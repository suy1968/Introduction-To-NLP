{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Word Tokenization Techniques in NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is word tokenization?\n",
    "Tokenization is breaking the raw text into small chunks. Tokenization breaks the raw text into words, sentences called **tokens**. These tokens help in understanding the context or developing the model for the NLP. \n",
    "The tokenization helps in interpreting the meaning of the text by analyzing the sequence of the words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, the text “It is raining” can be tokenized into ‘It’, ‘is’, ‘raining’"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization can be done to either separate words or sentences.\n",
    "- If the text is split into words using some separation technique it is called **word tokenization** and same separation done for sentences is called **sentence tokenization.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'everyone', '.', 'Welcome', 'to', 'my', 'Jupyter', 'Notebook']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simple Example of word tokenization\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#input a text\n",
    "text = \"Hello everyone. Welcome to my Jupyter Notebook\"\n",
    "word_tokenize(text)  #print tokenized words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'everyone', '.']\n",
      "['Welcome', 'to', 'the', 'NLP', 'Subject', '.']\n",
      "['Mr.Suyash', 'and', 'Tejas', 'Pandey', 'are', 'waiting', 'for', 'you', '.']\n",
      "['They', \"'ll\", 'join', 'you', 'soon', '.']\n"
     ]
    }
   ],
   "source": [
    "#import sent_tokenize from nltk library\n",
    "from nltk import sent_tokenize \n",
    "text = \"Hello everyone. Welcome to the NLP Subject. Mr.Suyash  and Tejas Pandey are waiting for you. They'll join you soon.\"\n",
    "for t in sent_tokenize(text):\n",
    "    \n",
    "    x =word_tokenize(t)\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop words : \n",
    "Stop words are those words in the text which does not add any meaning to the sentence and their removal will not affect the processing of text for the defined purpose. They are removed from the vocabulary to reduce noise and to reduce the dimension of the feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'Vineet', 'raj', 'Parashar', ',', 'Topper', 'of', 'my', 'Batch']\n",
      "['This', 'Vineet', 'raj', 'Parashar', ',', 'Topper', 'Batch']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "example_sent = \"This is a Vineet raj Parashar,Topper of my Batch\"\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "word_tokens = word_tokenize(example_sent)\n",
    "\n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "\n",
    "filtered_sentence = []\n",
    "\n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "\n",
    "print(word_tokens)\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Various Tokenization Techniques:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Whitespace Tokenization\n",
    "This is the simplest tokenization technique. Given a sentence or paragraph it tokenizes into words by splitting the input whenever a white space in encountered. This is the fastest tokenization technique but will work for languages in which the white space breaks apart the sentence into meaningful words.\n",
    "#### Syntax : tokenize.WhitespaceTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also import WhitespaceTokenizer() method from nltk\n",
    "from nltk.tokenize import WhitespaceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tejas', 'is', 'a', 'best', 'coder', 'in', 'juet', 'guna']\n"
     ]
    }
   ],
   "source": [
    "# Create a reference variable for Class WhitespaceTokenizer\n",
    "wtk = WhitespaceTokenizer()\n",
    "\n",
    "#give string input\n",
    "text1 = \"Tejas is a best coder in juet guna\"\n",
    "\n",
    "#use tokenize method\n",
    "tokens = wtk.tokenize(text1)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Dictionary Based Tokenization\n",
    "In this method the tokens are found based on the tokens already existing in the dictionary. If the token is not found, then special rules are used to tokenize it. It is an advanced technique compared to whitespace tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Vegetable'], ['Fruit'], ['Dry-fruit']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a list and dictionary variable\n",
    "suy = ['Tomato', 'Orange','Almond']\n",
    "kp = {'Vegetable': 'Tomato', 'Fruit': 'Orange','Dry-fruit':'Almond'}\n",
    "\n",
    "#extract words from dictonary items\n",
    "word2index = {key: val for val, key in kp.items()}\n",
    "\n",
    "#print tokenized words\n",
    "tokenized = [[word2index[word] for word in text.split()] for text in suy]\n",
    "tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Rule Based Tokenization\n",
    "In this technique a set of rules are created for the specific problem. The tokenization is done based on the rules. For example creating rules bases on grammar for particular language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Expression Tokenizer\n",
    "This technique uses regular expression to control the tokenization of text into tokens. Regular expression can be simple to complex and sometimes difficult to comprehend. This technique should be preferred when the above methods does not serve the required purpose. **It is a rule based tokenizer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Let's\", 'see', 'how', \"it's\", 'working', 'for', 'GUI', 'Python']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import RegexpTokenizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "  \n",
    "tk = RegexpTokenizer(\"[\\w']+\")  #[\\w']+ is one type of regular expression which extracts whole words from text.\n",
    "\n",
    "#give an input string\n",
    "text = \"Let's see how it's working for GUI Python\"\n",
    "tokens = tk.tokenize(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punctuation-based tokenizer\n",
    "Punctuation-based tokenization splits on whitespace and punctuations and also retains the punctuations.Punctuation-based tokenization overcomes the issue above and provides a meaningful token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mrs',\n",
       " '.',\n",
       " 'Surbhi',\n",
       " 'buys',\n",
       " 'Fruits',\n",
       " ':',\n",
       " 'Mango',\n",
       " ',',\n",
       " 'Banana',\n",
       " ',',\n",
       " 'Orange',\n",
       " ',',\n",
       " 'Cheery']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import wordpunct_tokenize from nltk\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "text = \"Mrs.Surbhi buys Fruits : Mango,Banana,Orange,Cheery \"\n",
    "\n",
    "tokens = wordpunct_tokenize(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweet Tokenizer\n",
    "Special texts, like Twitter tweets, have a characteristic structure and the generic tokenizers mentioned above fail to produce viable tokens when applied to these datasets. NLTK offers a special tokenizer for tweets to help in this case. This is a **rule-based tokenizer** that can remove HTML code, remove problematic characters, remove Twitter handles, and normalize text length by reducing the occurrence of repeated letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[':', 'NLP', 'is', 'way', 'tooo', 'coool', 'in', 'Reading', ':-)', ':-P', '<3']\n"
     ]
    }
   ],
   "source": [
    "#import TweetTokenizer from nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "#create object of tokenizer\n",
    "tknzr = TweetTokenizer(strip_handles=True)\n",
    "tweet= \" @NLP_learner: NLP is way tooo coool in Reading:-) :-P <3\"\n",
    "\n",
    "x= tknzr.tokenize(tweet)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MWE(Multi-Word Expression) Tokenizer\n",
    "The multi-word expression tokenizer is a rule-based, “add-on” tokenizer offered by NLTK. Once the text has been tokenized by a tokenizer of choice, some tokens can be re-grouped into multi-word expressions.\n",
    "- MWETokenizer takes a string and merges multi-word expressions into single tokens, using a lexicon of MWEs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What', 'is', 'M_W_E', 'in', 'Natural_Language_Processing']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import MWETokenizer\n",
    "   \n",
    "# Create a reference variable for Class MWETokenizer\n",
    "tk = MWETokenizer([('M', 'W', 'E'), ('Multi', 'Word', 'Tokenier')])\n",
    "tk.add_mwe(('Natural', 'Language', 'Processing'))\n",
    "   \n",
    "# Create a string input\n",
    "text = \"What is M W E in Natural Language Processing\"\n",
    "   \n",
    "# Use tokenize method\n",
    "tokenized = tk.tokenize(text.split())\n",
    "   \n",
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4)Penn TreeBank/Default Tokenization\n",
    "Tree bank is a corpus created which gives the semantic and syntactical annotation of language. Penn Treebank is one of the largest treebanks which was published. This technique of tokenization separates the punctuation, clitics (words that occur along with other words like I’m, don’t) and hyphenated words together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['That', \"'s\", 'True', ',', 'Mr.', 'Vineet', 'raj', 'parashar']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import tokenizer from nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "#create object of TreebankWordTokenizer\n",
    "tk = TreebankWordTokenizer()\n",
    "text = \"That's True, Mr. Vineet raj parashar\"\n",
    "tokens = tk.tokenize(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Subword Tokenization\n",
    "This tokenization is very useful for specific application where sub words make significance. In this technique the most frequently used words are given unique ids and less frequent words are split into sub words and they best represent the meaning independently. For example if the word few is appearing frequently in the text it will be assigned a unique id, where fewer and fewest which are rare words and are less frequent in the text will be split into sub words like few, er, and est. This helps the language model not to learn fewer and fewest as two separate words. This allows to identify the unknown words in the data set during training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> There are different types of subword tokenization and they are given below:\n",
    "\n",
    "- Byte-Pair Encoding (BPE)\n",
    "- WordPiece\n",
    "- Unigram Language Model\n",
    "- SentencePiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### i) Byte-Pair Encoding(BPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BPE uses Huffman encoding for tokenization meaning it uses more embedding or symbols for representing less frequent words and less symbols or embedding for more frequently used words.\n",
    "The BPE tokenization is bottom up sub word tokenization technique. The steps involved in BPE algorithm is given below.\n",
    "1. Split the words in the corpus into characters after appending </w>\n",
    "2. Initialize the vocabulary with unique characters in the corpus\n",
    "3. Compute the frequency of a pair of characters or character sequences in corpus\n",
    "4. Merge the most frequent pair in corpus\n",
    "5. Save the best pair to the vocabulary\n",
    "6. Repeat steps 3 to 5 for a certain number of iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ii) WordPiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WordPiece is similar to BPE techniques expect the way the new token is added to the vocabulary. BPE considers the token with most frequent occurring pair of symbols to merge into the vocabulary. While WordPiece considers the frequency of individual symbols also and based on below count it merges into the vocabulary.\n",
    "- Count (x, y) = frequency of (x, y) / frequency (x) * frequency (y)\n",
    "- The pair of symbols with maximum count will be considered to merge into vocabulary. So it allows rare tokens to be included into vocabulary as compared to BPE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### iii) Unigram Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast to BPE or WordPiece, Unigram initializes its base vocabulary to a large number of symbols and progressively trims down each symbol to obtain a smaller vocabulary. The base vocabulary could for instance correspond to all pre-tokenized words and the most common substrings. Unigram is not used directly for any of the models in the transformers, but it’s used in conjunction with SentencePiece."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### iv) SentencePiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing which treats the input as a raw input stream, thus including the space in the set of characters to use. It then uses the BPE or unigram algorithm to construct the appropriate vocabulary.\n",
    "- All tokenization algorithms described so far have the same problem: It is assumed that the input text uses spaces to separate words. However, not all languages use spaces to separate words.To solve this problem more generally Sentence Piece was intoduced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Chunking in NLP?\n",
    "Chunking in NLP is a process to take small pieces of information and group them into large units. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Split: ['learn', 'DSA', 'from', 'SDE', 'sheet', 'of', 'love', 'babbar', 'and', 'make', 'study', 'easy']\n",
      "After Token: [('learn', 'JJ'), ('DSA', 'NNP'), ('from', 'IN'), ('SDE', 'NNP'), ('sheet', 'NN'), ('of', 'IN'), ('love', 'NN'), ('babbar', 'NN'), ('and', 'CC'), ('make', 'VB'), ('study', 'NN'), ('easy', 'JJ')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk learn/JJ)\n",
      "  (mychunk DSA/NNP)\n",
      "  from/IN\n",
      "  (mychunk SDE/NNP sheet/NN)\n",
      "  of/IN\n",
      "  (mychunk love/NN babbar/NN and/CC)\n",
      "  make/VB\n",
      "  (mychunk study/NN easy/JJ))\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk import RegexpParser\n",
    "text =\"learn DSA from SDE sheet of love babbar and make study easy\".split()\n",
    "print(\"After Split:\",text)\n",
    "tokens_tag = pos_tag(text)\n",
    "print(\"After Token:\",tokens_tag)\n",
    "patterns= \"\"\"mychunk:{<NN.?>*<VBD.?>*<JJ.?>*<CC>?}\"\"\"\n",
    "chunker = RegexpParser(patterns)\n",
    "print(\"After Regex:\",chunker)\n",
    "output = chunker.parse(tokens_tag)\n",
    "print(\"After Chunking\",output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['learn', 'DSA', 'from', 'SDE', 'sheet', 'of', 'love', 'babbar']\n",
      "[('learn', 'JJ'), ('DSA', 'NNP'), ('from', 'IN'), ('SDE', 'NNP'), ('sheet', 'NN'), ('of', 'IN'), ('love', 'NN'), ('babbar', 'NN')]\n",
      "(S\n",
      "  learn/JJ\n",
      "  DSA/NNP\n",
      "  from/IN\n",
      "  SDE/NNP\n",
      "  (NP sheet/NN)\n",
      "  of/IN\n",
      "  (NP love/NN)\n",
      "  (NP babbar/NN))\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "text = \"learn DSA from SDE sheet of love babbar \"\n",
    "tokens = nltk.word_tokenize(text)\n",
    "print(tokens)\n",
    "tag = nltk.pos_tag(tokens)\n",
    "print(tag)\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "cp  =nltk.RegexpParser(grammar)\n",
    "result = cp.parse(tag)\n",
    "print(result)\n",
    "result.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THANK YOU"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
